# ğŸš€ AI Model Gateway - AI Model Serving & Secure Gateway

## Overview
The **AI Model Gateway** is a **FastAPI-based model serving gateway** designed to securely handle AI model execution requests across distributed **Kubernetes clusters**. It provides a **scalable, secure, and efficient way to manage AI model inference**, ensuring authentication, authorization, caching, and robust pre/post-processing mechanisms.

ğŸ”¹ **Key Features:**
- **ğŸš€ AI Model Gateway** â€“ Routes inference requests to ML models deployed in Kubernetes clusters.
- **ğŸ” Authentication & Authorization** â€“ Validates requests via JWT & API tokens.
- **ğŸ›  Data Transformation & Processing** â€“ Prepares input/output for seamless model execution.
- **âš¡ Caching for Low Latency** â€“ Implements Redis-based caching to optimize performance.
- **ğŸŒ Multi-Model Orchestration** â€“ Manages multiple models efficiently across clusters.
- **ğŸ›¡ï¸ Secure API Proxying** â€“ Uses NGINX for request routing & load balancing.

---

## ğŸ“Œ Architecture & Components
### **1ï¸âƒ£ Secure Model Gateway (FastAPI + NGINX)**
- Handles **incoming model inference requests**.
- Routes traffic securely via **NGINX reverse proxy**.
- Supports **multi-tenant AI inference execution**.

### **2ï¸âƒ£ Authentication & Authorization**
- **User authentication via JWT-based tokens** (`validate_auth_token_util.py`).
- **Authorization to prevent unauthorized model access**.

### **3ï¸âƒ£ AI Model Execution Pipeline**
- **Preprocessing layer**: Normalizes & transforms input (`transform_input_data_for_model_util.py`).
- **Model execution handler**: Routes calls to the correct ML model.
- **Post-processing layer**: Converts outputs into structured responses.

### **4ï¸âƒ£ High-Performance Caching with Redis**
- **Speeds up model responses** for frequently accessed data.
- Implemented in **`redis_feature_plugin.py`**.

### **5ï¸âƒ£ Secure Model Deployment & Routing (NGINX + Docker)**
- Uses **NGINX reverse proxy** (`vps-nginx/nginx.conf`).
- Deploys containerized **FastAPI app** via **Docker** (`vps-model-gateway/Dockerfile`).

### **6ï¸âƒ£ Kubernetes-Ready for Large-Scale AI Inference**
- Designed to run in **distributed Kubernetes clusters**.
- Ensures **scalability & load balancing** for AI model execution.

---

## ğŸš€ Quick Start Guide
### **1ï¸âƒ£ Clone the Repository**
```sh
git clone https://github.com/vinay-jayanna/ai-model-gateway.git
cd ai-model-gateway
```

### **2ï¸âƒ£ Install Dependencies**
```sh
pip install -r vps-model-gateway/requirements.txt
```

### **3ï¸âƒ£ Run the FastAPI Application**
```sh
uvicorn src.main:app --host 0.0.0.0 --port 8000 --reload
```

### **4ï¸âƒ£ Start the NGINX Reverse Proxy**
```sh
docker build -t vps-nginx ./vps-nginx
docker run -p 8080:80 vps-nginx
```

### **5ï¸âƒ£ Run Tests**
```sh
pytest vps-model-gateway/tests
```

---

## ğŸ”¥ Why Use This?
ğŸ”¹ **Enterprise-Grade AI Model Serving** â€“ Secure, scalable, and Kubernetes-ready.  
ğŸ”¹ **Optimized for Low Latency AI Inference** â€“ Redis caching speeds up responses.  
ğŸ”¹ **Multi-Model Support** â€“ Easily orchestrate and manage multiple AI models.  
ğŸ”¹ **Production-Ready API Gateway** â€“ Secure model execution with JWT authentication.  
ğŸ”¹ **NGINX-Powered Load Balancing** â€“ Ensures optimal performance and availability.  

---

## ğŸ“š Additional Resources
- **ğŸ”— Model Routing & Authentication:** [`src/utils/`](vps-model-gateway/src/utils)
- **ğŸ“¦ Model Execution Services:** [`src/services/`](vps-model-gateway/src/services)
- **âš¡ Test Cases for API & Models:** [`tests/`](vps-model-gateway/tests)
- **ğŸ›  NGINX Configuration for Secure Routing:** [`vps-nginx/nginx.conf`](vps-nginx/nginx.conf)


